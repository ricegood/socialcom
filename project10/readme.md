# :pencil: Text Analysis

## :pencil2:Homework 1

#### I.​ '서울대', '서울대학교'를 각각 이용하여 query했을 때 나오는 상위 5권의 책들

##### [서울대]

|       |           name            |      id      |                             text                             |
| :---: | :-----------------------: | :----------: | :----------------------------------------------------------: |
| 21356 | 당신의 말이 당신을 말한다 | 9.788966e+12 | 서울대 강의 유정아 품격 소통 이야기 방송인 이자 서울대 수업 강사 저자 현장 이론... |
| 15824 |        20대 심리학        | 9.788926e+12 | 심리학 은 년 서울대 생 사이 입 소문 신입생 반드시 추천 강의 이자 서울대 최고 ... |
|  70   |     밀란 쿤데라 읽기      | 9.788937e+12 | 쿤데라 전집 완간 특별 해설 집 간 쪽 상당 이 해설 집 계간 세계 문학 편집 위원... |
| 18743 |        신화와 영화        | 9.788995e+12 | 이 책 저자 년 서울대 대학원 서양고전학 협동 과정 홈페이지 게재 몇 편의 글 최근... |
|  553  |          도련님           | 9.788997e+12 | 서울대 인문과학 연구소 청소년 필독 나 메 소세키 가장 사랑 대표 작 이 책 일본 ... |

##### [서울대학교]

|       |                name                |      id      |                             text                             |
| :---: | :--------------------------------: | :----------: | :----------------------------------------------------------: |
| 7653  |    유정아의 서울대 말하기 강의     | 9.788955e+12 | 자신 바로 타인 제대로 소통 위해 방법 준 저자 서울대학교 년 개설 후 사랑 온 강... |
| 5874  |              파우스트              | 9.788931e+12 | 년 서울대학교 권장 도서 서울대 권장 도서 권 괴테 희곡 집 괴테 자신 삶 세계관 ... |
| 6500  | 나는 자기계발서를 읽고 벤츠를 샀다 | 9.788955e+12 | 꿈 성공 법 도대체 누구 경영학 교수 나 꿈 법 자기계발 에서 인간 세상 인 문서 ... |
| 16786 |  반야의 지혜를 먹고 자라는 아이들  | 9.788987e+12 | 불 길 학부모 가장 문제 무엇 내 자녀 잠재 가능성 불성 씨앗 알 싹 틔워 열매 촉... |
| 2621  |         서울대 명품 강의 2         | 9.788994e+12 | 서울대학교 명강 사진 한국 사회 뇌관 년 서울대 사회과학 연구원 개설 제 기 시민 ... |

#### a. 다른 결과가 나오는 이유

일단 키워드 자체가 다르기 때문이다. '서울대' 키워드는 20062번째 키워드고, '서울대학교'는 20063번째 키워드이다. 따라서 '서울대학교'키워드 안에 '서울대'라는 단어가 포함되어있다고 해도, 이러한 관계를 전혀 반영하지 못한다.

#### b. 결과를 같게 하려면 어떻게 해야 하는가

위에서 언급한 것처럼, '서울대학교'키워드 안에 '서울대'라는 단어가 포함되어 있으므로, 이를 query할 경우 20062번째 키워드인 '서울대'가 포함되도록 (query문의 transform을 통해 생성된 sparse matrix에 (0,20063)뿐 아니라 (0,20062) 값도 1이 되도록) 처리해야 할 것이다.



#### II. Query를 tf-idf로 바꾸어서 검색할 경우

예를들어, '개념 원리' 단어에 대해 Count Vectorizer를 썼을 때와 TF-IDF Vectorizer를 사용했을 때의 가중치와 쿼리 결과 차이를 살펴보면 다음과 같다.

##### 가중치

```python
# Count Vectorizer
(0, 903)	1 # 개념
(0, 29203)	1 # 원리

# TF-IDF Vectorizer
(0, 903)	0.681054720027568 # 개념
(0, 29203)	0.7322325234023486 # 원리
```

##### [Count Vectorizer: "개념 원리"]

|       |            name            |      id      |                             text                             |
| :---: | :------------------------: | :----------: | :----------------------------------------------------------: |
| 6470  |        개념-뿌리들         | 9.788977e+12 | 소운 이정우 저작 집 의 권 개념 뿌리 은 년 간 개념 뿌리 권 합본 책 철학 공부... |
| 13867 |      헤르메스 가르침       | 9.791185e+12 | 이 책 모든 신비 사상 아버지 점성술 설립 연금술 창안 헤르메스 핵심 가르침 헤르메... |
| 16340 | 그림으로 이해하는 경제사상 | 9.788958e+12 | 인간 그 자체 하나 경제 행위 여러 경제 개념 용어 이 책 현대 의미 경제 개념 중... |
| 1333  |    노아가 동물을 태워요    | 9.788904e+12 | 사물 세상 구체 개념 유아 가장 개념 책 시리즈 개념 하나님 로부터 것 하나님 창조... |
| 6430  | 예수님이 아픈사람을 고쳐요 | 9.788904e+12 | 사물 세상 구체 개념 유아 가장 개념 책 시리즈 개념 하나님 로부터 것 하나님 창조... |
| 11373 |    다윗이 양들을 돌봐요    | 9.788904e+12 | 사물 세상 구체 개념 유아 가장 개념 책 시리즈 개념 하나님 로부터 것 하나님 창조... |
| 13857 |      베드로는 기뻐요       | 9.788904e+12 | 사물 세상 구체 개념 유아 가장 개념 책 시리즈 개념 하나님 로부터 것 하나님 창조... |
| 2015  |      하나님의 학습법       | 9.788953e+12 | 성경 자녀 교육법 오늘날 한국 사회 가장 문제 교육 입시 위주 파행 교육 그 도 과... |
| 12855 |      회사 개념어 사전      | 9.788966e+12 | 개념 바로 일이 바로 선 고객 보고 업 본질 전략 경영 과 관리 당신 조직 개념 어... |
| 18387 |       과학실험 큐티        | 9.788904e+12 | 이 책 아이 수 투명인간 원리 내비게이션 원리 관성 법칙 비행기 양력 등 평소 과학... |

##### [TF-IDF Vectorizer : "개념 원리"]

|       |            name            |      id      |                             text                             |
| :---: | :------------------------: | :----------: | :----------------------------------------------------------: |
| 13867 |      헤르메스 가르침       | 9.791185e+12 | 이 책 모든 신비 사상 아버지 점성술 설립 연금술 창안 헤르메스 핵심 가르침 헤르메... |
| 6470  |        개념-뿌리들         | 9.788977e+12 | 소운 이정우 저작 집 의 권 개념 뿌리 은 년 간 개념 뿌리 권 합본 책 철학 공부... |
| 2015  |      하나님의 학습법       | 9.788953e+12 | 성경 자녀 교육법 오늘날 한국 사회 가장 문제 교육 입시 위주 파행 교육 그 도 과... |
| 16340 | 그림으로 이해하는 경제사상 | 9.788958e+12 | 인간 그 자체 하나 경제 행위 여러 경제 개념 용어 이 책 현대 의미 경제 개념 중... |
| 1333  |    노아가 동물을 태워요    | 9.788904e+12 | 사물 세상 구체 개념 유아 가장 개념 책 시리즈 개념 하나님 로부터 것 하나님 창조... |
| 6430  | 예수님이 아픈사람을 고쳐요 | 9.788904e+12 | 사물 세상 구체 개념 유아 가장 개념 책 시리즈 개념 하나님 로부터 것 하나님 창조... |
| 11373 |    다윗이 양들을 돌봐요    | 9.788904e+12 | 사물 세상 구체 개념 유아 가장 개념 책 시리즈 개념 하나님 로부터 것 하나님 창조... |
| 13857 |      베드로는 기뻐요       | 9.788904e+12 | 사물 세상 구체 개념 유아 가장 개념 책 시리즈 개념 하나님 로부터 것 하나님 창조... |
| 18387 |       과학실험 큐티        | 9.788904e+12 | 이 책 아이 수 투명인간 원리 내비게이션 원리 관성 법칙 비행기 양력 등 평소 과학... |
| 12855 |      회사 개념어 사전      | 9.788966e+12 | 개념 바로 일이 바로 선 고객 보고 업 본질 전략 경영 과 관리 당신 조직 개념 어... |

#### a. 다른 결과가 나오는 이유

다른 결과가 나오는 이유는, 단순히 Count Vectorizer를 사용한다면 '개념', '원리' 단어에 대한 가중치가 동일하게 1이지만, **TF-IDF Transform**을 거칠 경우 단어가 등장하는 빈도를 사용하여 **단어의 중요도를 고려해 가중치**가 계산되므로 '개념'의 경우 0.68, **'원리'의 경우 0.73의 더 높은 가중치**를 갖게되기 때문이다.

실제로, **두 결과에서 달라진 점은 '하나님의 학습법', '과학실험 큐티'의 순위가 올라간 것**인데, 두 책 모두 **'개념'이라는 단어는 없고 '원리'라는 단어가 많이 포함된 경우**이다.

#### b. 각각의 장단점

Count Vectorizer의 경우 단어가 등장하는 빈도에 따른 중요성을 고려할 수 없다는 단점이 있지만 TF-IDF와같은 다른 Vectorize방법에 비해 단순하여 시간에 대한 비용이 적다는 장점이 있을 수 있을 것이다. 반면 TF-IDF의 경우 이러한 Count Vectorizer의 단점을 개선하여 단어의 중요도를 고려해 가중치가 계산된다는 장점이 있다.



## :pencil2:Homework 2

### Identify Authors with Naive Bayesian Classifier

#### Most Informative Features

상위 20개의 important features는 다음과 같다.

|   1    |     2      |   3    |   4    |   5    |        6        |      7      |    8     |     9     |   10   |
| :----: | :--------: | :----: | :----: | :----: | :-------------: | :---------: | :------: | :-------: | :----: |
| access | commit-tee | draft  |   's   | export | adminis-tration |   credit    | federal  | companies |  fed   |
| **11** |   **12**   | **13** | **14** | **15** |     **16**      |   **17**    |  **18**  |  **19**   | **20** |
| online |  congress  |  said  |   ''   |   ``   |      banks      | encryp-tion | internet |     .     |   ,    |

#### 이를 개선하기 위해선?

수업 자료의 코드를 통해 Classification을 수행할 경우 accuracy가 0.6472 나오는 것을 확인할 수 있었다. 이를 개선하기 위해, Vectorizer와 Transformer의 파라미터 값을 수정해가며 텍스트 feature를 바꿔보았다. 이를 통해 accuracy를 최대 **0.7268**까지 향상시킬 수 있었다.

이를 위해서 아래와 같은 알고리즘의 개선이 있었다.

1) **stopwords를 고려하지 않았다**

* stopwords를 제거할 경우 오히려 정확도가 감소하는 것을 확인할 수 있었다. 자주 사용되지만 큰 의미는 없는 단어의 모음집인 stopwords가 오히려 작가의 글쓰기 특징을 반영한다고 추측해볼 수 있다.

2) `CountVectorizer` 함수의 파라미터에 `ngram_range = (1, 3)` 옵션을 둠으로써 한 단어만을 고려하는 것이 아닌 **연속된 두 단어, 세 단어의 조합도 고려**했다

* 이를 통해 정확도가 크게 향상하였는데, 작가의 글쓰기 특징을 반영할 수 있는 표현이 단순히 한 단어가 아닌 숙어와같이 두 단어, 세 단어의 조합일 수 있기 때문인 것으로 생각된다.

3) **feature 수를 3,000개에서 80,000개로 늘렸다**

* 위와 같이 한 단어뿐 아니라 연속된 두, 세 단어의 조합도 고려하다보니 전체 feature의 수가 증가하였는데, 이 중 3,000개만을 고려하는 것은 너무 제한된 데이터였다. 따라서 문서에 대한 충분한 정보를 제공할 수 있도록 80,000개의 feature를 고려하여 정확도를 향상시켰다.

4) `TfidfTransformer` 함수의 파라미터에 `sublinear_tf = True` 옵션을 둠으로써 **높은 TF값들에 대해 smoothing처리**를 하였다

* 이는 아웃라이어를 제거하여 정확도를 향상시키는 효과가 있었다.
