{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Example using Traning set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Loading Train Dataset\n",
    "'''\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "DATA_DIR = 'data/C50/C50train'\n",
    "\n",
    "articles = []\n",
    "authors = []\n",
    "\n",
    "for authorname in os.listdir(DATA_DIR):\n",
    "    if authorname.startswith('.'):\n",
    "        continue\n",
    "    author_dir = os.path.join(DATA_DIR, authorname)\n",
    "    for filename in os.listdir(author_dir):\n",
    "        if not filename.endswith('.txt'):\n",
    "            continue\n",
    "        filepath = os.path.join(author_dir, filename)\n",
    "        file = open(filepath, 'r')\n",
    "        text = file.read()\n",
    "        articles.append(text)\n",
    "        authors.append(authorname)\n",
    "        \n",
    "authors = np.array(authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Document -> Tf-idf\n",
    "'''\n",
    "\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "#vectorizer = CountVectorizer(min_df=2, stop_words='english', tokenizer=nltk.word_tokenize, max_features=3000)\n",
    "vectorizer = CountVectorizer(min_df=2, tokenizer=nltk.word_tokenize, ngram_range = (1, 3), max_features=80000)\n",
    "count_features = vectorizer.fit_transform(articles)\n",
    "\n",
    "#transformer = TfidfTransformer()\n",
    "transformer = TfidfTransformer(sublinear_tf = True)\n",
    "tfidf_features = transformer.fit_transform(count_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.04217813]]\n",
      "[1195 1159 1189 1199]\n",
      "['RobinSidel' 'RobinSidel' 'RobinSidel' 'RobinSidel']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Document Similarity 계산 예제\n",
    "'''\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 문서간 거리를 잰다.\n",
    "# e.g. 0과 1의 similarity\n",
    "print(cosine_similarity(tfidf_features[1195], tfidf_features[1]))\n",
    "\n",
    "# e.g.1195번째 아이와 가장 비슷한 글 TOP4\n",
    "cosine_similarities = cosine_similarity(tfidf_features[1195], tfidf_features) # 1195와 나머지전체에대한 similarity\n",
    "related_docs_indices = cosine_similarities.argsort()[0]\n",
    "print(related_docs_indices[:-5:-1]) # begin:end:stop\n",
    "print(authors[related_docs_indices[:-5:-1]]) # 맨앞에는 원작자, 그 뒤는 비슷한 글 쓴사람들"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Loading Test Dataset\n",
    "'''\n",
    "\n",
    "DATA_DIR = 'data/C50/C50test'\n",
    "\n",
    "test_corpus = []\n",
    "test_authors = []\n",
    "\n",
    "for authorname in os.listdir(DATA_DIR):\n",
    "    if authorname.startswith('.'):\n",
    "        continue\n",
    "    author_dir = os.path.join(DATA_DIR, authorname)\n",
    "    for filename in os.listdir(author_dir):\n",
    "        if not filename.endswith('.txt'):\n",
    "            continue\n",
    "        filepath = os.path.join(author_dir, filename)\n",
    "        file = open(filepath, 'r')\n",
    "        text = file.read()\n",
    "        test_corpus.append(text)\n",
    "        test_authors.append(authorname)\n",
    "        \n",
    "test_authors = np.array(test_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Label Encoding\n",
    "'''\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "labels = label_encoder.fit_transform(authors)\n",
    "test_labels = label_encoder.transform(test_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Train Naive Bayesian Classifier\n",
    "'''\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB(alpha=0.01).fit(tfidf_features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7268"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Test Classifier with All Test Dataset\n",
    "'''\n",
    "test_counts = vectorizer.transform(test_corpus)\n",
    "test_tfidf = transformer.transform(test_counts)\n",
    "classifier.score(test_tfidf, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top\n",
      "that -7.589123179817156\n",
      "association -7.550885470559749\n",
      "'' -7.513471363975897\n",
      "`` -7.508719965946974\n",
      "said -7.498146462752036\n",
      "in -7.473058024567271\n",
      "federal -7.468570594133816\n",
      "and -7.422707727197643\n",
      "banks -7.39160737414375\n",
      "online -7.3431551980342284\n",
      "a -7.340998695467735\n",
      "congress -7.287753464976623\n",
      "to -7.271270185164254\n",
      "of -7.247128806142091\n",
      "encryption -7.236134611632388\n",
      ". -7.136179974898932\n",
      ", -7.079580129827989\n",
      "the -7.016485992773483\n",
      "the internet -6.990050008164698\n",
      "internet -6.80142025904969\n",
      "\n",
      "Bottom\n",
      "! -12.147437463428275\n",
      "! service -12.147437463428275\n",
      "$ 0.07 -12.147437463428275\n",
      "$ 0.08 -12.147437463428275\n",
      "$ 0.10 -12.147437463428275\n",
      "$ 0.10 to -12.147437463428275\n",
      "$ 0.14 -12.147437463428275\n",
      "$ 0.15 -12.147437463428275\n",
      "$ 0.17 -12.147437463428275\n",
      "$ 0.18 -12.147437463428275\n",
      "$ 0.19 -12.147437463428275\n",
      "$ 0.20 -12.147437463428275\n",
      "$ 0.21 -12.147437463428275\n",
      "$ 0.22 -12.147437463428275\n",
      "$ 0.29 -12.147437463428275\n",
      "$ 0.30 -12.147437463428275\n",
      "$ 0.33 -12.147437463428275\n",
      "$ 0.35 -12.147437463428275\n",
      "$ 0.36 -12.147437463428275\n",
      "$ 0.39 -12.147437463428275\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Important Features\n",
    "'''\n",
    "n = 20\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "topn = sorted(zip(classifier.coef_[0], feature_names))[-n:]\n",
    "bottomn = sorted(zip(classifier.coef_[0], feature_names))[:n]\n",
    "\n",
    "print(\"Top\")\n",
    "for coef, feature in topn:\n",
    "    print(feature, coef)\n",
    "    \n",
    "print(\"\\nBottom\")\n",
    "for coef, feature in bottomn:\n",
    "    print(feature, coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[47  0  0 ...  0  0  0]\n",
      " [ 0 30  0 ...  0  0  0]\n",
      " [ 0  0 24 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 46  0  0]\n",
      " [ 0  0  0 ...  0 25  0]\n",
      " [ 0  0  0 ...  0  0 21]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Confusion Matrix\n",
    "'''\n",
    "predicted_labels = classifier.predict(test_tfidf)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(test_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11  4]\n",
      "['GrahamEarnshaw' 'BernardHickey']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[6.30545512e-03, 2.08021440e-02, 2.74904530e-02, 3.42058319e-03,\n",
       "        4.97121099e-02, 6.66911245e-02, 6.71803535e-03, 4.02381960e-03,\n",
       "        6.77594610e-03, 6.85772783e-02, 2.18180202e-02, 1.03771736e-01,\n",
       "        1.59297965e-02, 4.95393677e-03, 3.35593147e-03, 5.31474840e-03,\n",
       "        7.11464870e-03, 7.66082238e-03, 3.12675117e-02, 9.14398145e-03,\n",
       "        1.85950499e-03, 2.72297626e-02, 6.35627096e-02, 6.72767580e-03,\n",
       "        5.77898393e-03, 3.02125576e-02, 3.56480992e-02, 2.21146442e-02,\n",
       "        2.86874946e-03, 2.90749496e-03, 7.69847728e-03, 3.10817611e-02,\n",
       "        4.06042380e-03, 3.13534211e-02, 1.64056376e-02, 3.01760157e-02,\n",
       "        6.84779345e-03, 5.36348828e-03, 5.07956987e-03, 2.30718438e-02,\n",
       "        4.28481157e-03, 4.77377001e-02, 2.11853692e-02, 2.77124247e-03,\n",
       "        7.96276706e-03, 4.74721141e-03, 5.82312416e-02, 1.18517477e-02,\n",
       "        5.85237864e-03, 4.47883376e-03],\n",
       "       [1.93298861e-03, 1.71263229e-02, 4.99039506e-03, 4.73034748e-04,\n",
       "        5.92312020e-01, 1.58581549e-02, 1.09179505e-02, 3.87269503e-04,\n",
       "        1.07006287e-02, 7.55158433e-03, 2.05004298e-03, 1.11674410e-02,\n",
       "        2.43708303e-03, 1.61257743e-03, 1.46763901e-03, 6.35615372e-03,\n",
       "        2.08286462e-03, 1.55500455e-02, 5.18481272e-03, 1.51947472e-02,\n",
       "        1.88133467e-03, 4.75042370e-02, 1.02306090e-02, 1.85753102e-02,\n",
       "        3.86987749e-03, 1.15106358e-02, 2.65383435e-03, 2.76877188e-03,\n",
       "        1.79090691e-02, 1.60674901e-03, 6.03089521e-02, 4.76636600e-03,\n",
       "        4.18341759e-03, 7.76373187e-03, 6.14592991e-03, 3.07736854e-03,\n",
       "        7.37162478e-03, 4.33761382e-04, 4.34525348e-03, 2.13332468e-03,\n",
       "        3.95010473e-04, 8.70709467e-03, 4.82969046e-03, 2.05097245e-03,\n",
       "        5.31015731e-03, 5.09485359e-03, 9.98956413e-03, 1.36913961e-02,\n",
       "        2.24808298e-03, 3.28926255e-03]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Toy Example\n",
    "'''\n",
    "sample_doc = ['Strong business fundamentals', 'Solid growth in the Commonwealth Bank']\n",
    "sample_count = vectorizer.transform(sample_doc)\n",
    "sample_tfidf = transformer.transform(sample_count)\n",
    "predicted = classifier.predict(sample_tfidf)\n",
    "print(predicted)\n",
    "print(label_encoder.inverse_transform(predicted))\n",
    "classifier.predict_proba(sample_tfidf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
